import streamlit as st
import pandas as pd
import numpy as np
from gensim.models import Word2Vec
from sklearn.cluster import KMeans
import os

# Configure Streamlit page
st.set_page_config(
    page_title="Mental Health Text Clustering",
    page_icon="üß†",
    layout="centered"
)

# Define stopword bigrams
STOPWORD_BIGRAMS = {
    "yang tidak", "tidak ada", "aku tidak", "tidak enak", "ada yang", 
    "tidak yang", "orang orang", "teman teman", "gara gara", "https chat", 
    "assalamu alaikum", "afc", "whatsapp com"
}

# Cluster mapping (model output -> your desired output)
CLUSTER_MAPPING = {
    0: 2,  # model cluster 0 -> your cluster 2s
    1: 3,  # model cluster 1 -> your cluster 3
    2: 1,  # model cluster 2 -> your cluster 1
    3: 0   # model cluster 3 -> your cluster 0
}

# Cluster topics
CLUSTER_TOPICS = {
    0: "Psychosomatics and early treatment of stress",
    1: "Meaning-seeking and alternative coping strategies", 
    2: "Existential concerns intertwined with religious and social pressures",
    3: "Emotional expression and relationship difficulties"
}

def remove_stopword_bigrams(text):
    """Remove stopword bigrams from text"""
    if not isinstance(text, str):
        return ""
    text = text.lower()
    for bigram in STOPWORD_BIGRAMS:
        text = text.replace(bigram, "")
    return text.strip()

def preprocess_text(text):
    """Preprocess input text"""
    cleaned_text = remove_stopword_bigrams(text)
    tokens = cleaned_text.split()
    return tokens

def document_vector(doc, w2v_model):
    """Convert document to vector using Word2Vec model"""
    try:
        vectors = [w2v_model.wv[word] for word in doc if word in w2v_model.wv]
        if vectors:
            return np.mean(vectors, axis=0)
        else:
            # For Gensim 3.8.0, use wv.vector_size instead of vector_size
            return np.zeros(w2v_model.wv.vector_size)
    except AttributeError:
        # Fallback for different Gensim versions
        try:
            return np.zeros(w2v_model.vector_size)
        except:
            return np.zeros(100)  # Default fallback

@st.cache_data
def load_cluster_data():
    """Load cluster CSV files"""
    try:
        all_data = []
        for i in range(4):
            file = f"cluster_{i}_resultsV7.csv"
            if os.path.exists(file):
                df = pd.read_csv(file)
                all_data.append(df)
        
        if all_data:
            combined_df = pd.concat(all_data, ignore_index=True)
            return combined_df, True
        return None, False
    except Exception as e:
        st.error(f"Error loading cluster data: {e}")
        return None, False

@st.cache_resource
def load_models():
    """Load Word2Vec model and create KMeans"""
    try:
        # Load Word2Vec model
        w2v_model = Word2Vec.load("word2vec_mental_health.model")
        
        # Load cluster data and create KMeans
        cluster_df, _ = load_cluster_data()
        if cluster_df is None:
            return None, None, False
        
        # Check if 'tokenized' column exists
        if 'tokenized' not in cluster_df.columns:
            st.error("‚ùå 'tokenized' column not found in cluster data")
            return None, None, False
        
        # Prepare training data
        texts = cluster_df['tokenized'].fillna('').astype(str).tolist()
        texts = [remove_stopword_bigrams(text.lower()) for text in texts]
        tokenized_texts = [text.split() for text in texts if text.strip()]
        
        if not tokenized_texts:
            st.error("‚ùå No valid tokenized texts found")
            return None, None, False
        
        # Vectorize documents
        X_vectors = np.array([document_vector(doc, w2v_model) for doc in tokenized_texts])
        non_zero_mask = ~np.all(X_vectors == 0, axis=1)
        X_vectors_clean = X_vectors[non_zero_mask]
        
        if len(X_vectors_clean) == 0:
            st.error("‚ùå No valid document vectors found")
            return None, None, False
        
        # Create KMeans model
        kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
        kmeans.fit(X_vectors_clean)
        
        return w2v_model, kmeans, True
        
    except Exception as e:
        st.error(f"Error loading models: {e}")
        return None, None, False

def main():
    st.title("üß† Mental Health Text Classification")
    st.markdown("### Classify Indonesian mental health text into thematic categories")
    
    # Load models
    w2v_model, kmeans_model, models_loaded = load_models()
    
    if not models_loaded:
        st.error("‚ùå Could not load models. Please ensure all required files are present.")
        st.markdown("""
        **Required files:**
        - `word2vec_mental_health.model`
        - `cluster_0_resultsV7.csv`
        - `cluster_1_resultsV7.csv`
        - `cluster_2_resultsV7.csv`
        - `cluster_3_resultsV7.csv`
        """)
        st.stop()
    
    st.success("‚úÖ Models loaded successfully!")
    
    # Text input
    st.subheader("üìù Enter Text for Classification")
    user_text = st.text_area(
        "Type Indonesian text here:",
        height=120,
        placeholder="Masukkan teks bahasa Indonesia di sini..."
    )
    
    # Classify button
    if st.button("üîç Classify", type="primary"):
        if user_text.strip():
            try:
                # Preprocess text
                tokens = preprocess_text(user_text)
                
                if len(tokens) == 0:
                    st.warning("‚ö†Ô∏è No valid words found after preprocessing.")
                    return
                
                # Convert to vector
                doc_vector = document_vector(tokens, w2v_model)
                
                if np.all(doc_vector == 0):
                    st.warning("‚ö†Ô∏è No words from your text found in vocabulary.")
                    return
                
                # Predict cluster (model output)
                model_cluster = kmeans_model.predict([doc_vector])[0]
                
                # Map to your desired cluster
                final_cluster = CLUSTER_MAPPING[model_cluster]
                
                # Get topic
                topic = CLUSTER_TOPICS[final_cluster]
                
                # Display result
                st.subheader("üìä Classification Result")
                
                # Show the mapped cluster and topic
                st.markdown(f"**Cluster:** {final_cluster}")
                st.markdown(f"**Topic:** {topic}")
                
            except Exception as e:
                st.error(f"‚ùå Error during classification: {e}")
                
        else:
            st.warning("‚ö†Ô∏è Please enter some text to classify.")
    
    # Show cluster information
    st.markdown("---")
    st.subheader("üìã Cluster Topics")
    
    for cluster_id, topic in CLUSTER_TOPICS.items():
        st.markdown(f"**Cluster {cluster_id}:** {topic}")

if __name__ == "__main__":
    main()